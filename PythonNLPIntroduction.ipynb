{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data (adapted from ImportingDataIntoPandas)\n",
    "import pandas as pd\n",
    "\n",
    "#Note that for this to work the NoMoreSilence_ProjectData.tsv file needs to be\n",
    "#in the same directory (folder) that this notebook file is in, and that you started\n",
    "#the jupyter notebook from.\n",
    "\n",
    "df = pd.read_csv('NoMoreSilence_ProjectData.tsv', sep='\\t')\n",
    "\n",
    "#this is creating a variable of all the sources, which we'll use to experiment with pulling out\n",
    "#just the call number for each collection. \n",
    "\n",
    "#the for loop will iterate through each source, and use the .split method to create a new list\n",
    "#with each element (separated by commas, which we specified with .split(', ') -- note \n",
    "# comma then space) as a list item.\n",
    "\n",
    "#this threw an error, because one of the entries was a float not a string. We have to choose to \n",
    "#either make it a string or to ignore it. In this code I've made it a string with source = str(source)\n",
    "#but it may actually be better to ignore it. (with an if else statement)\n",
    "\n",
    "sources = df['Source']\n",
    "for source in sources:\n",
    "    source = str(source)\n",
    "    s_list = source.split(', ')\n",
    "    \n",
    "#Filling out the above to more completely get the call number. Note that this time we are opting to skip\n",
    "#the row if it has no data for the source, this is contained in the \"if type(source) == str:\"\n",
    "\n",
    "collection_list = []\n",
    "for source in sources:\n",
    "    if type(source) == str:\n",
    "        source_list = source.split(', ')\n",
    "        try:\n",
    "            if source_list[1] == '':\n",
    "                collection_list.append(source_list[2])\n",
    "            else:\n",
    "                collection_list.append(source_list[1])\n",
    "        except IndexError:\n",
    "            collection_list.append('no data')\n",
    "\n",
    "#the below makes a set from the list, to pull out all the unique values so we can see what the extent of the values\n",
    "#we're getting. \n",
    "#we can see that there are some duplicates due to trailing spaces, so we'll need to fix that. \n",
    "            \n",
    "collection_set = set(collection_list)\n",
    "\n",
    "#We're almost there, but we want the code to remove trailing spaces and to replace spaces with dashes for cleaner data.\n",
    "#The below does that.\n",
    "\n",
    "collection_list = []\n",
    "for source in sources:\n",
    "    call_no = 'blank'\n",
    "    if type(source) == str:\n",
    "        source_list = source.split(', ')\n",
    "        try:\n",
    "            if source_list[1] == '':\n",
    "                if source_list[2][-1] == ' ':\n",
    "                    call_no = source_list[2][0:-1].replace(' ', '-')\n",
    "                else:\n",
    "                    call_no = source_list[2].replace(' ', '-')\n",
    "            else:\n",
    "                if source_list[1][-1] == ' ':\n",
    "                    call_no = source_list[1][0:-1].replace(' ', '-')\n",
    "                else:\n",
    "                    call_no = source_list[1].replace(' ', '-')\n",
    "        except IndexError:\n",
    "            call_no = 'no-data'\n",
    "    collection_list.append(call_no)\n",
    "    \n",
    "collection_set = set(collection_list)\n",
    "\n",
    "#Now we need to take the code above and turn it into a function that will run on the \"Source\" field\n",
    "#for every line in the dataframe. We need to define its inputs a little differently, and do the function\n",
    "#definition.\n",
    "\n",
    "def get_call_no(row):\n",
    "    call_no = 'blank'\n",
    "    if type(row['Source']) == str:\n",
    "        source_list = row['Source'].split(', ')\n",
    "        try:\n",
    "            if source_list[1] == '':\n",
    "                if source_list[2][-1] == ' ':\n",
    "                    call_no = source_list[2][0:-1].replace(' ', '-')\n",
    "                else:\n",
    "                    call_no = source_list[2].replace(' ', '-')\n",
    "            else:\n",
    "                if source_list[1][-1] == ' ':\n",
    "                    call_no = source_list[1][0:-1].replace(' ', '-')\n",
    "                else:\n",
    "                    call_no = source_list[1].replace(' ', '-')\n",
    "        except IndexError:\n",
    "            call_no = 'no-data'\n",
    "    return call_no\n",
    "\n",
    "#This used the function we just defined above to go through each row in the dataframe and pull out the call_no\n",
    "#and put it into a new column called 'call_no', which we've defined simply by naming it in the 'df['call_no'] = ...'\n",
    "\n",
    "df['call_no'] = df.apply(lambda row: get_call_no(row), axis=1)\n",
    "\n",
    "#using the df.unique method, we can check the same thing we did above using set() -- that there are no repeat values. \n",
    "\n",
    "call_nums = df['call_no'].unique()\n",
    "\n",
    "#and now we can sort it by collection simply by creating a variable that defines all the rows that match a certain \n",
    "#collection value, and passing this as a selection of the df variable:\n",
    "\n",
    "act_up = df['call_no'] == 'MSS-98-47'\n",
    "df[act_up]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns to get rid of some spaces.\n",
    "df.columns = [x.strip(' ') for x in list(df.columns)]\n",
    "\n",
    "#Grab one document identifier.\n",
    "document_id = df[act_up].iloc[0]['Local Identifier']\n",
    "\n",
    "#Grab that document.\n",
    "document = df.loc[df['Local Identifier'] == document_id]\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the column \"Ocr text\" (OCR stands for optical character recognition)\n",
    "#For more info see here: https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa\n",
    "#Anyone notice any issues?\n",
    "str(document[\"Ocr text\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to add spaces between words! But how do we know what is and isn't a word?\n",
    "document_text = document[\"Ocr text\"].values[0]\n",
    "\n",
    "#First, what language is this text?\n",
    "from langdetect import detect\n",
    "detect(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One way would be to get a list of \"all\" words from GitHub.\n",
    "#https://github.com/dwyl/english-words\n",
    "import requests\n",
    "\n",
    "def load_words():\n",
    "    target_url = 'https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt'\n",
    "    response = requests.get(target_url)\n",
    "    data = response.text\n",
    "    valid_words = set(data.split())\n",
    "    \n",
    "    return valid_words\n",
    "\n",
    "#english_words = load_words()\n",
    "#print(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oh no! It didn't work!\n",
    "#What else could we do?\n",
    "#Maybe NLP?\n",
    "import nltk\n",
    "\n",
    "#This will open a Window to download files for the NLTK package.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's split sentences and then do words, one chunk at a time.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Get a sentence!\n",
    "sentences = sent_tokenize(document_text)\n",
    "\n",
    "#Some look good and others... Not so much.\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get one sentence.\n",
    "example_sentence = sentences[5]\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to split apart the words.\n",
    "words = word_tokenize(example_sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try a spell checker again, but make it more efficient.\n",
    "#https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words\n",
    "import wordninja\n",
    "\n",
    "spell_checked_words = wordninja.split(example_sentence)\n",
    "print(spell_checked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not perfect, but definitely better!\n",
    "#Let's put the sentence back together:\n",
    "\n",
    "corrected_sentence = \" \".join(spell_checked_words)\n",
    "print(corrected_sentence)\n",
    "\n",
    "#There are better ways to do this that employ deep learning models\n",
    "#but they are a bit too complicated to go into detail here.\n",
    "#For example: https://github.com/atpaino/deep-text-corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can we find out anything else from a sentence?\n",
    "\n",
    "#Let's look at grammar first (parts of speech, specifically).\n",
    "nltk.pos_tag(word_tokenize(corrected_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What do these abbreviations mean?\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also look at the sentence's sentiment.\n",
    "#That is, positivity, negativity, or neutrality.\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "pol_score = sia.polarity_scores(corrected_sentence)\n",
    "print(pol_score)\n",
    "\n",
    "#Seems like it is mostly neutral!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go through and correct each sentence of our OCR.\n",
    "corrected_ocr = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    spell_checked_words = wordninja.split(sentence)\n",
    "    corrected_sentence = \" \".join(spell_checked_words)\n",
    "    corrected_ocr.append(corrected_sentence)\n",
    "    \n",
    "print(corrected_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wow that's pretty long... Can we summarize it?\n",
    "#Well yes! But it's complicated.\n",
    "#Great step-by-step tutorial here:\n",
    "#https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "#But we'll use a pre-existing package, sumy.\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 1\n",
    "\n",
    "full_text = \". \".join(corrected_ocr)\n",
    "parser = PlaintextParser.from_string(full_text, Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizer = Summarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there other summary methods?\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer as Edmundson\n",
    "from sumy.summarizers.luhn import LuhnSummarizer as Luhn\n",
    "\n",
    "print(\"\\nLuhn:\")\n",
    "summarizer = Luhn(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slightly better! Turns out text quality is pretty important here.\n",
    "\n",
    "#Now let's do sentiment analysis on each sentence.\n",
    "sia = SIA()\n",
    "polarity_results = []\n",
    "for sentence in corrected_ocr:\n",
    "    pol_score = sia.polarity_scores(sentence)\n",
    "    pol_score[\"sentence\"] = sentence\n",
    "    polarity_results.append(pol_score)\n",
    "    \n",
    "#Load into dataframe.\n",
    "sentences_df = pd.DataFrame.from_records(polarity_results)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#We can even do this with the entire document.\n",
    "#But... It might take a while.\n",
    "\n",
    "#Let's try something simpler with our corrected text.\n",
    "#Let's stem all the words in the doc.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = []\n",
    "\n",
    "for sentence in corrected_ocr:\n",
    "    for words in word_tokenize(sentence):\n",
    "            stemmed_words.append(ps.stem(words))\n",
    "                \n",
    "print(stemmed_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we have a lot that doesn't tell us anything... yet.\n",
    "\n",
    "#Let's start by removing the numbers.\n",
    "without_numbers = []\n",
    "\n",
    "for word in stemmed_words:\n",
    "    new_string = \"\".join([x for x in word if not x.isdigit()])\n",
    "    if new_string:\n",
    "        without_numbers.append(new_string)\n",
    "        \n",
    "print(without_numbers[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let's make everything lowercase.\n",
    "lowercase_words = [x.lower() for x in without_numbers]\n",
    "print(lowercase_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's look at word frequency.\n",
    "unique_words = set(lowercase_words)\n",
    "word_freq = {}\n",
    "\n",
    "for word in unique_words:\n",
    "    word_freq[word] = lowercase_words.count(word)\n",
    "    \n",
    "#Sorted by frequency.\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_word_freq[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice anything?\n",
    "#We need to remove a few things still...\n",
    "\n",
    "#Like punctuation:\n",
    "import string\n",
    "without_punctuation = []\n",
    "\n",
    "for word in lowercase_words:\n",
    "    new_string = \"\".join([x for x in word if x not in string.punctuation])\n",
    "    if new_string:\n",
    "        without_punctuation.append(new_string)\n",
    "        \n",
    "unique_words = set(without_punctuation)\n",
    "word_freq = {}\n",
    "for word in unique_words:\n",
    "    word_freq[word] = without_punctuation.count(word)\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_word_freq[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And single letters:\n",
    "no_single_letters = []\n",
    "\n",
    "for word in without_punctuation:\n",
    "    if len(word) > 1:\n",
    "        no_single_letters.append(word)\n",
    "        \n",
    "unique_words = set(no_single_letters)\n",
    "word_freq = {}\n",
    "for word in unique_words:\n",
    "    word_freq[word] = no_single_letters.count(word)\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_word_freq[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And stopwords:\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "no_stopwords = []\n",
    "\n",
    "for word in no_single_letters:\n",
    "    if word not in stopwords.words('english'):\n",
    "        no_stopwords.append(word)\n",
    "        \n",
    "unique_words = set(no_stopwords)\n",
    "word_freq = {}\n",
    "for word in unique_words:\n",
    "    word_freq[word] = no_stopwords.count(word)\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "top_fifty = sorted_word_freq[:50]\n",
    "print(top_fifty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize this a bit.\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(background_color = \"white\", max_words = 500)\n",
    "wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe this isn't giving as much info as we'd like...\n",
    "#Let's do a barchart instead.\n",
    "plt.title(\"Word Frequencies\")\n",
    "plt.ylabel(\"# of Occurrences\")\n",
    "plt.xlabel(\"Word\")\n",
    "\n",
    "plt.bar([i[0] for i in top_fifty], [i[1] for i in top_fifty])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay, now we've got the basics!\n",
    "#Let's move on to vectorization.\n",
    "#We'll start with bag of words.\n",
    "\n",
    "#Actually! We already did that. This is a big of words here:\n",
    "print(top_fifty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#However, what we really want is this part:\n",
    "print([i[1] for i in top_fifty])\n",
    "\n",
    "#This is our vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whereas this, is our vocabulary;\n",
    "print([i[0] for i in top_fifty])\n",
    "\n",
    "#What information do we lose by vectorizing?\n",
    "#How could we try to retain that knowledge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So do we have to do all of the above steps for every document?\n",
    "#Thankfully, not really! sklearn and nltk do a lot of that for us.\n",
    "#(Except the OCR correction... We still have to do that mostly ourselves.)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1,1), tokenizer=token.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make sure we remember to use wordninja!\n",
    "\n",
    "#Let's pick 50 documents from two collections to compare.\n",
    "#Which two collections should we use?\n",
    "#It could be interesting to compare a government/legal\n",
    "#collection to a more personal collection.\n",
    "\n",
    "unique_collect = set(list(df['call_no']))\n",
    "collection_freq = {}\n",
    "for collect in unique_collect:\n",
    "    collection_freq[collect] = list(df['call_no']).count(collect)\n",
    "    \n",
    "print(collection_freq)\n",
    "\n",
    "#Let's use:\n",
    "#  2000-46 - AIDS Legal Referral Panel Records (85 records)\n",
    "#  2003-09 - Linda Alband Collection of Randy Shilts Materials (209 records)\n",
    "\n",
    "aids_legal_referral_papers = df['call_no'] == '2000-46'\n",
    "linda_alband_papers = df['call_no'] == '2003-09'\n",
    "\n",
    "aids_legal_referral_papers_50_random = df[aids_legal_referral_papers].sample(n = 50)\n",
    "linda_alband_papers_50_random = df[linda_alband_papers].sample(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's correct these.\n",
    "corrected_ocr_aids_legal = []\n",
    "for x in aids_legal_referral_papers_50_random[\"Ocr text\"]:\n",
    "    sentences = sent_tokenize(document_text)\n",
    "    corrected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        spell_checked_words = wordninja.split(sentence)\n",
    "        corrected_sentence = \" \".join(spell_checked_words)\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    corrected_ocr_aids_legal.append(\" \".join(corrected_sentences))\n",
    "    \n",
    "corrected_ocr_linda_alband = []\n",
    "for x in linda_alband_papers_50_random[\"Ocr text\"]:\n",
    "    sentences = sent_tokenize(document_text)\n",
    "    corrected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        spell_checked_words = wordninja.split(sentence)\n",
    "        corrected_sentence = \" \".join(spell_checked_words)\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    corrected_ocr_linda_alband.append(\" \".join(corrected_sentences))\n",
    "    \n",
    "#Let's look at a couple.\n",
    "print(corrected_ocr_linda_alband[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we'll paste these to our small dataframes.\n",
    "aids_legal_referral_papers_50_random[\"CorrectedOCR\"] = corrected_ocr_aids_legal\n",
    "linda_alband_papers_50_random[\"CorrectedOCR\"] = corrected_ocr_linda_alband\n",
    "\n",
    "#And then we'll add a column with \"0\" for dritz_selma and \"1\" for sue_rochman.\n",
    "aids_legal_referral_papers_50_random[\"Class\"] = [0] * 50\n",
    "linda_alband_papers_50_random[\"Class\"] = [1] * 50\n",
    "\n",
    "#Let's look at a couple.\n",
    "print(linda_alband_papers_50_random[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we'll paste these two dataframes together.\n",
    "complete_df = pd.concat([aids_legal_referral_papers_50_random, linda_alband_papers_50_random], axis=0)\n",
    "print(complete_df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's finish preprocessing by vectorizing!\n",
    "text_counts = cv.fit_transform(complete_df['CorrectedOCR'])\n",
    "\n",
    "print(text_counts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's split our data into a training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts,\n",
    "    complete_df['Class'],\n",
    "    test_size = 0.3,\n",
    "    random_state = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we'll use a multinomial Naive Bayes classifier to see how well we can predict which set a document is in.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whoops... That's not so great (unless you got a great random split).\n",
    "#Feel free to experiment with different test_size parameters!\n",
    "\n",
    "#Here we'll try TF-IDF instead!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "text_tf = tf.fit_transform(complete_df['CorrectedOCR'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, \n",
    "    complete_df['Class'], \n",
    "    test_size = 0.3,\n",
    "    random_state = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if we change the classifier type?\n",
    "#SGD Classifier?\n",
    "#How many iterations should we use?\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(max_iter=20, tol=1e-3).fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"SGDClassifier Accuracy:\", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We could also try a couple more!\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#PassiveAggressiveClassifier\n",
    "clf = PassiveAggressiveClassifier(tol=1e-3).fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"PassiveAggressiveClassifier Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "#Perceptron\n",
    "clf = Perceptron(tol=1e-3).fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Perceptron Accuracy:\", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfortunately, we probably don't have much time to continue\n",
    "#this, but with more documents and corrected text, we\n",
    "#could summarize whole collections, predict if a document\n",
    "#belongs to a collection, or even generate completely new\n",
    "#documents! See here:\n",
    "#https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
